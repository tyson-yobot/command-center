✅ STEP 1: Add Simple In-Memory Vector Store
Create: utils/vectorStore.ts

ts
Copy
Edit
type VectorEntry = {
  chunk: string;
  vector: number[];
};

const vectorDB: VectorEntry[] = [];

export const storeVector = (entry: VectorEntry) => {
  vectorDB.push(entry);
};

export const getVectors = () => vectorDB;

export const cosineSimilarity = (a: number[], b: number[]) => {
  const dot = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const normA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const normB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dot / (normA * normB);
};
✅ STEP 2: Update ragUpload.ts to Store Chunks
Update the server/ragUpload.ts file (edit just the bottom half):

Replace this section:

ts
Copy
Edit
    // Embed each chunk
    const embeddings = [];
    for (const chunk of chunks) {
      const embeddingRes = await openai.embeddings.create({
        model: 'text-embedding-ada-002',
        input: chunk,
      });
      const vector = embeddingRes.data[0].embedding;
      embeddings.push({ chunk, vector });
    }
With this:

ts
Copy
Edit
    // Embed + store
    const { storeVector } = await import('../utils/vectorStore');
    let count = 0;

    for (const chunk of chunks) {
      const embeddingRes = await openai.embeddings.create({
        model: 'text-embedding-ada-002',
        input: chunk,
      });

      const vector = embeddingRes.data[0].embedding;
      storeVector({ chunk, vector });
      count++;
    }

    console.log(`✅ Stored ${count} embedded chunks.`);
    res.json({ status: 'success', chunks: count });
✅ STEP 3: Create Retrieval Route
Create: server/ragSearch.ts

ts
Copy
Edit
import express from 'express';
import { OpenAI } from 'openai';
import { getVectors, cosineSimilarity } from '../utils/vectorStore';

const router = express.Router();
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

router.post('/search', async (req, res) => {
  try {
    const { question } = req.body;
    if (!question) return res.status(400).json({ error: 'Question required' });

    const embeddingRes = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: question,
    });

    const queryVector = embeddingRes.data[0].embedding;
    const allVectors = getVectors();

    const ranked = allVectors
      .map(entry => ({
        ...entry,
        score: cosineSimilarity(queryVector, entry.vector),
      }))
      .sort((a, b) => b.score - a.score)
      .slice(0, 5);

    const context = ranked.map(r => r.chunk).join('\n---\n');

    const aiRes = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: `You are a helpful assistant answering based on the following knowledge base.`,
        },
        {
          role: 'user',
          content: `Context:\n${context}\n\nQuestion:\n${question}`,
        },
      ],
    });

    const answer = aiRes.choices?.[0]?.message?.content || 'No answer available.';
    res.json({ answer });
  } catch (err) {
    console.error('RAG Search Error:', err);
    res.status(500).json({ error: 'Search failed' });
  }
});

export default router;
✅ STEP 4: Mount Route in server/index.ts
Add this line:

ts
Copy
Edit
import ragSearchRouter from './ragSearch';
app.use('/api/rag', ragSearchRouter); // Already exists for upload, adds /search
You now have two live endpoints:

POST /api/rag/upload → upload + embed chunks

POST /api/rag/search → enter a question, return answer from docs

✅ STEP 5: Test Retrieval via POST
Send this body to test:

json
Copy
Edit
{
  "question": "What are our safety protocols for new hires?"
}
Response:

json
Copy
Edit
{
  "answer": "According to the uploaded documents, new hires must complete OSHA forms..."
}