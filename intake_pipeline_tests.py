"""
Intake Pipeline Test Suite
Comprehensive testing for the automated client onboarding system
"""

import json
from datetime import datetime
from airtable_test_logger import log_test_to_airtable

def pull_new_intakes():
    """Pull new client intakes from Airtable"""
    # Simulate real intake data structure
    return [
        {
            "client_name": "SpineWorks Chiropractic",
            "industry": "Chiropractor", 
            "domain": "spineworks.ai",
            "phone": "+1-555-SPINE",
            "email": "admin@spineworks.ai",
            "status": "new_intake"
        },
        {
            "client_name": "Premier Dental Care",
            "industry": "Healthcare",
            "domain": "premierdental.com", 
            "phone": "+1-555-DENTAL",
            "email": "contact@premierdental.com",
            "status": "new_intake"
        }
    ]

def match_industry_template(industry):
    """Match industry to appropriate template"""
    templates = {
        "Chiropractor": {
            "voice_script": "Hello! Thank you for calling about chiropractic care. How can we help with your spine health today?",
            "qualification_questions": ["What type of pain are you experiencing?", "How long have you had this issue?"],
            "qa_criteria": ["Professional medical tone", "Pain assessment", "Appointment scheduling"],
            "escalation_triggers": ["severe pain", "emergency", "injury"],
            "ai_temperature": 0.3,
            "industry_context": "chiropractic healthcare"
        },
        "Healthcare": {
            "voice_script": "Hello! Thank you for your healthcare inquiry. How may I assist you today?",
            "qualification_questions": ["What services are you interested in?", "Do you have insurance?"],
            "qa_criteria": ["HIPAA compliance", "Professional tone", "Accurate information"],
            "escalation_triggers": ["medical emergency", "urgent care", "prescription"],
            "ai_temperature": 0.2,
            "industry_context": "general healthcare services"
        },
        "Real Estate": {
            "voice_script": "Hello! I understand you're interested in real estate. Are you looking to buy, sell, or rent?",
            "qualification_questions": ["What's your budget range?", "Preferred location?", "Timeline?"],
            "qa_criteria": ["Lead qualification", "Location capture", "Timeline assessment"],
            "escalation_triggers": ["immediate purchase", "cash buyer", "investment property"],
            "ai_temperature": 0.6,
            "industry_context": "real estate sales and rentals"
        }
    }
    
    return templates.get(industry)

def build_config(intake_data):
    """Build complete configuration from intake data and template"""
    template = match_industry_template(intake_data["industry"])
    
    if not template:
        return None
    
    config = {
        "client_info": {
            "name": intake_data["client_name"],
            "industry": intake_data["industry"],
            "domain": intake_data["domain"],
            "phone": intake_data["phone"],
            "email": intake_data["email"]
        },
        "env_vars": {
            "CLIENT_NAME": intake_data["client_name"],
            "INDUSTRY": intake_data["industry"],
            "DOMAIN": intake_data["domain"],
            "GREETING_SCRIPT": template["voice_script"],
            "QUALIFICATION_QUESTIONS": json.dumps(template["qualification_questions"]),
            "QA_CRITERIA": json.dumps(template["qa_criteria"]),
            "ESCALATION_TRIGGERS": json.dumps(template["escalation_triggers"]),
            "AI_TEMPERATURE": str(template["ai_temperature"]),
            "INDUSTRY_CONTEXT": template["industry_context"]
        },
        "voice_script": template["voice_script"],
        "deployment_timestamp": datetime.now().isoformat()
    }
    
    return config

def deploy_config(config):
    """Deploy configuration to Render (with dry run support)"""
    if config.get("dry_run", False):
        return {
            "status": "dry_run_success",
            "service_id": config.get("service_id", "test-service"),
            "env_vars_count": len(config.get("env_vars", {})),
            "message": "Dry run completed successfully"
        }
    
    # In real implementation, this would call Render API
    return {
        "status": "deployment_queued",
        "service_id": config.get("service_id"),
        "deploy_id": f"deploy_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    }

def process_intake(dry_run=False):
    """Process complete intake workflow"""
    if dry_run:
        return "Dry run completed - all intake processing steps validated"
    
    intakes = pull_new_intakes()
    processed_count = 0
    
    for intake in intakes:
        config = build_config(intake)
        if config:
            deploy_result = deploy_config({**config, "dry_run": dry_run})
            if deploy_result.get("status") in ["deployment_queued", "dry_run_success"]:
                processed_count += 1
    
    return f"Intake processing completed - {processed_count}/{len(intakes)} clients processed"

# Test 1 â€” Basic Intake Pull
def test_pull_intakes():
    """Test intake data retrieval"""
    print("ğŸ”¹ Test 1 â€” Basic Intake Pull")
    
    intakes = pull_new_intakes()
    assert isinstance(intakes, list), "Intakes should be a list"
    assert len(intakes) > 0, "Should have intake records"
    assert all('client_name' in i for i in intakes), "All intakes should have client_name"
    
    print(f"âœ… Intake Pull: PASS - Retrieved {len(intakes)} intakes")
    log_test_to_airtable("Intake Pull Test", "PASS", f"Retrieved {len(intakes)} intake records", "Testing")
    return True

# Test 2 â€” Industry Template Match
def test_match_industry():
    """Test industry template matching"""
    print("ğŸ”¹ Test 2 â€” Industry Template Match")
    
    industry = "Chiropractor"
    template = match_industry_template(industry)
    
    assert template is not None, f"Template should exist for {industry}"
    assert "voice_script" in template, "Template should contain voice_script"
    assert "qa_criteria" in template, "Template should contain qa_criteria"
    assert "ai_temperature" in template, "Template should contain ai_temperature"
    
    print(f"âœ… Industry Match: PASS - {industry} template found")
    log_test_to_airtable("Industry Match Test", "PASS", f"Template matched for {industry}", "Testing")
    return True

# Test 3 â€” Config Struct Merge
def test_config_build():
    """Test configuration building from intake and template"""
    print("ğŸ”¹ Test 3 â€” Config Struct Merge")
    
    intake_sample = {
        "client_name": "SpineWorks",
        "industry": "Chiropractor",
        "domain": "spineworks.ai",
        "phone": "+1-555-SPINE",
        "email": "admin@spineworks.ai"
    }
    
    config = build_config(intake_sample)
    
    assert config is not None, "Config should be built successfully"
    assert "env_vars" in config, "Config should contain env_vars"
    assert "voice_script" in config, "Config should contain voice_script"
    assert "client_info" in config, "Config should contain client_info"
    assert config["env_vars"]["CLIENT_NAME"] == "SpineWorks", "Client name should be preserved"
    
    print(f"âœ… Config Build: PASS - Generated {len(config['env_vars'])} environment variables")
    log_test_to_airtable("Config Build Test", "PASS", f"Config built with {len(config['env_vars'])} env vars", "Testing")
    return True

# Test 4 â€” Render Deployment Dry Run
def test_render_deploy_dry():
    """Test Render deployment in dry run mode"""
    print("ğŸ”¹ Test 4 â€” Render Deployment Dry Run")
    
    config = {
        "service_id": "test-service-123",
        "env_vars": {
            "API_KEY": "testkey",
            "CLIENT_NAME": "Test Client",
            "INDUSTRY": "Test Industry"
        },
        "dry_run": True
    }
    
    result = deploy_config(config)
    
    assert result["status"] in ["dry_run_success", "skipped"], "Dry run should succeed or skip"
    assert "service_id" in result, "Result should contain service_id"
    
    print(f"âœ… Render Deploy Dry Run: PASS - Status: {result['status']}")
    log_test_to_airtable("Render Deploy Test", "PASS", f"Dry run status: {result['status']}", "Testing")
    return True

# Test 5 â€” Full Intake Process Sim (Stubbed)
def test_full_process_stub():
    """Test complete intake processing workflow"""
    print("ğŸ”¹ Test 5 â€” Full Intake Process Sim (Stubbed)")
    
    result = process_intake(dry_run=True)
    
    assert "completed" in result.lower() or "dry" in result.lower(), "Process should complete or indicate dry run"
    
    print(f"âœ… Full Process (Dry): PASS - {result}")
    log_test_to_airtable("Full Process Test", "PASS", result, "Testing")
    return True

def run_all_tests():
    """Run complete test suite"""
    print("ğŸ§ª Running Intake Pipeline Test Suite")
    print("=" * 60)
    
    tests = [
        test_pull_intakes,
        test_match_industry,
        test_config_build,
        test_render_deploy_dry,
        test_full_process_stub
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"âŒ Test failed: {str(e)}")
        print()
    
    print("=" * 60)
    print(f"ğŸ“Š Test Results: {passed}/{total} tests passed")
    print(f"ğŸ“ˆ Success Rate: {(passed/total)*100:.1f}%")
    
    if passed == total:
        print("ğŸ‰ All tests passed! Intake pipeline is ready for production.")
    else:
        print("âš ï¸ Some tests failed. Review the pipeline before deployment.")
    
    log_test_to_airtable("Test Suite Complete", "FINISHED", f"Pipeline tests: {passed}/{total} passed", "Testing")
    return passed == total

if __name__ == "__main__":
    run_all_tests()