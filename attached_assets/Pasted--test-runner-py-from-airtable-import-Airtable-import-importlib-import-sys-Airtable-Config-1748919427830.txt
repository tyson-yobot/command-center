# test_runner.py

from airtable import Airtable
import importlib
import sys

# Airtable Config
BASE_ID = "your_base_id"
TABLE_NAME = "Tests"
API_KEY = "your_airtable_key"

# Initialize Airtable
airtable = Airtable(BASE_ID, TABLE_NAME, API_KEY)


def run_tests_in_range(start, end, test_module_name):
    test_module = importlib.import_module(test_module_name)
    missing_tests = []

    for i in range(start, end + 1):
        func_name = f"test_{i:03}"
        test_func = getattr(test_module, func_name, None)

        if callable(test_func):
            print(f"\n▶️ Running Test {i:03}")
            try:
                test_func()
                airtable.insert({"test_number": i, "status": "pass"})
            except Exception as e:
                airtable.insert({"test_number": i, "status": f"fail: {str(e)[:100]}"})
                print(f"❌ Test {i:03} FAILED: {e}")
        else:
            print(f"❌ Test {i:03} MISSING")
            missing_tests.append(i)

    return missing_tests


def fetch_logged_tests():
    records = airtable.get_all(fields=["test_number"])
    return sorted(set(int(r["fields"]["test_number"]) for r in records if "test_number" in r["fields"]))


def detect_test_gaps(expected_range, logged_tests):
    missing = [t for t in expected_range if t not in logged_tests]
    if missing:
        print(f"\n🚨 MISSING TESTS: {missing}")
    else:
        print("\n✅ All expected tests accounted for.")
    return missing


def generate_audit_trail(logged_tests):
    with open("test_audit_log.txt", "w") as f:
        for t in sorted(logged_tests):
            f.write(f"Test {t:03} - Logged ✅\n")
    print("📜 Audit trail written to test_audit_log.txt")


if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("Usage: python test_runner.py <start> <end> <test_module>")
        sys.exit(1)

    start = int(sys.argv[1])
    end = int(sys.argv[2])
    module = sys.argv[3]

    missing_tests = run_tests_in_range(start, end, module)
    logged_tests = fetch_logged_tests()
    detect_test_gaps(range(start, end + 1), logged_tests)
    generate_audit_trail(logged_tests)
